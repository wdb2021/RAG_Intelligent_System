from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from session_manager import SessionManager
from chat_interface import ChatInterface
from config import configInstance  # ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®å®ä¾‹


def test_model_connection():
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸è°ƒç”¨"""
    try:
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ¨¡å‹å®ä¾‹
        test_llm = ChatOpenAI(
            openai_api_key=configInstance.openai_api_key,
            base_url=configInstance.model_api_url,
            model=configInstance.model_type,
            temperature=0,
            max_retries=1,
            timeout=10
        )

        # å‘é€æµ‹è¯•è¯·æ±‚
        response = test_llm.invoke("è¯·å›å¤'æœåŠ¡çŠ¶æ€æ­£å¸¸'")

        # éªŒè¯å“åº”ç»“æœ
        if "æ­£å¸¸" in response.content:
            print("âœ… æ¨¡å‹è¿æ¥æµ‹è¯•é€šè¿‡")
            print(response.content)
            return True
        else:
            print("âš ï¸ æ”¶åˆ°å¼‚å¸¸å“åº”:", response.content)
            return False

    except Exception as e:
        print("âŒ æ¨¡å‹è¿æ¥æµ‹è¯•å¤±è´¥:")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"è¯¦ç»†ä¿¡æ¯: {str(e)}")
        print("\nè¯·æ£€æŸ¥ä»¥ä¸‹é…ç½®:")
        print(f"1. API å¯†é’¥: {'å·²é…ç½®' if configInstance.openai_api_key else 'æœªé…ç½®'}")
        print(f"2. API ç«¯ç‚¹: {configInstance.model_api_url}")
        print(f"3. æ¨¡å‹ç±»å‹: {configInstance.model_type}")
        return False



def initialize_system():
    """ç³»ç»Ÿåˆå§‹åŒ–å‡½æ•°ï¼ˆå®Œå…¨åŸºäºé…ç½®ç±»ï¼‰"""

    # 1. åˆå§‹åŒ–å¸¦è‡ªå®šä¹‰ç«¯ç‚¹çš„æ¨¡å‹
    llm = ChatOpenAI(
        openai_api_key=configInstance.openai_api_key,  # ä»é…ç½®å®ä¾‹è·å–
        base_url=configInstance.model_api_url,  # è‡ªå®šä¹‰APIç«¯ç‚¹
        model=configInstance.model_type,  # é…ç½®ä¸­çš„æ¨¡å‹ç±»å‹
        temperature=0.7,
        timeout=30  # å»ºè®®æ·»åŠ è¶…æ—¶é…ç½®
    )

    # 2. æ„å»ºå¯ç»´æŠ¤çš„æç¤ºæ¨¡æ¿
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ])

    # 3. æ„å»ºå¯å¤ç”¨é“¾
    processing_chain = prompt_template | llm

    # 4. åˆå§‹åŒ–å¸¦éªŒè¯çš„ä¼šè¯ç®¡ç†å™¨
    session_manager = SessionManager()

    # 5. é…ç½®å†å²æ„ŸçŸ¥é“¾
    chain_with_history = RunnableWithMessageHistory(
        processing_chain,
        history_factory=lambda session_id: session_manager.get_history(session_id),
        input_messages_key="input",
        history_messages_key="history"
    )

    # 6. åˆ›å»ºå¯é…ç½®çš„äº¤äº’æ¥å£
    return ChatInterface(
        chain=chain_with_history,
        session_manager=session_manager,
        ai_prefix="åŠ©æ‰‹",
        user_prefix="ç”¨æˆ·",
        max_retries=3  # å¯æ·»åŠ åˆ°é…ç½®ç±»
    )


if __name__ == "__main__":
    # å…ˆæ‰§è¡Œè¿æ¥æµ‹è¯•
    if test_model_connection():
        # æµ‹è¯•é€šè¿‡ååˆå§‹åŒ–ç³»ç»Ÿ
        try:
            chat_system = initialize_system()
            print("\n" + "=" * 50)
            chat_system.run()
        except Exception as e:
            print(f"ç³»ç»Ÿè¿è¡Œæ—¶é”™è¯¯: {str(e)}")
            exit(1)
    else:
        print("ğŸ›‘ å¯åŠ¨ä¸­æ­¢ï¼Œè¯·å…ˆä¿®å¤é…ç½®é—®é¢˜")
        exit(1)