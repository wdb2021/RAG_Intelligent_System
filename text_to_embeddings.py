import os
import json
import nltk
import numpy as np
from sentence_transformers import SentenceTransformer

# ================= é…ç½®éƒ¨åˆ† =================
nltk_data_dir = os.path.expanduser("~/nltk_data")
os.makedirs(nltk_data_dir, exist_ok=True)
nltk.data.path.append(nltk_data_dir)  # å¼ºåˆ¶æŒ‡å®šæ•°æ®è·¯å¾„


# ================= æ ¸å¿ƒåŠŸèƒ½ =================
def split_into_chunks(text, max_sentences=3, max_chars=500):
    """æ™ºèƒ½åˆ†å—å‡½æ•°"""
    sentences = nltk.sent_tokenize(text)

    chunks = []
    current_chunk = []
    char_count = 0

    for sent in sentences:
        sent_len = len(sent)

        # æ»¡è¶³ä»»ä¸€æ¡ä»¶åˆ™æ–°å»ºå—
        if (len(current_chunk) >= max_sentences) or (char_count + sent_len > max_chars):
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            char_count = 0

        current_chunk.append(sent)
        char_count += sent_len

    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


# ================= æ‰§è¡Œæµç¨‹ =================
if __name__ == "__main__":
    # 1. è¯»å–æ–‡æœ¬
    with open("documents/read_en.txt", "r", encoding="utf-8") as f:
        text = f.read()

    # 2. åˆ†å—å¤„ç†
    chunks = split_into_chunks(text, max_sentences=3, max_chars=500)

    # 3. ç”Ÿæˆå‘é‡
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)

    # ================= éªŒè¯é€»è¾‘ =================
    # éªŒè¯åµŒå…¥ç»´åº¦ (384ç»´)
    assert embeddings.shape[1] == 384, f"ç»´åº¦é”™è¯¯ï¼å½“å‰ç»´åº¦ï¼š{embeddings.shape[1]}"
    print(f"\nâœ… éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰åµŒå…¥å‘é‡å‡ä¸º 384 ç»´")

    # ä¿å­˜ numpy æ ¼å¼
    os.makedirs("output", exist_ok=True)
    np.save("output/embeddings.npy", embeddings)
    print("âœ… å‘é‡å·²ä¿å­˜è‡³ output/embeddings.npy")

    # éªŒè¯ç¬¬ä¸€ä¸ª chunk çš„å¥å­æ•°é‡
    first_chunk = chunks[0]
    sentences_in_chunk = nltk.sent_tokenize(first_chunk)
    print("\nğŸ” é¦–å—å†…å®¹éªŒè¯ï¼š")
    print(f"æ–‡æœ¬å†…å®¹ï¼š\n{first_chunk}\n")
    print(f"åŒ…å«å¥å­æ•°ï¼š{len(sentences_in_chunk)}")
    print("å®é™…å¥å­åˆ—è¡¨ï¼š")
    for i, sent in enumerate(sentences_in_chunk, 1):
        print(f"{i}. {sent}")

    # ================= ä¿å­˜ç»“æ„åŒ–æ•°æ® =================
    output_data = [
        {
            "chunk_id": idx,
            "text": chunk,
            "embedding": embedding.tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨
            "char_count": len(chunk),
            "sentence_count": len(nltk.sent_tokenize(chunk))
        }
        for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings))
    ]

    with open("output/chunks.json", "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    print("\nâœ… ç»“æ„åŒ–æ•°æ®å·²ä¿å­˜è‡³ output/chunks.json")

# é¢„æœŸè¾“å‡ºç¤ºä¾‹
"""
âœ… éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰åµŒå…¥å‘é‡å‡ä¸º 384 ç»´
âœ… å‘é‡å·²ä¿å­˜è‡³ output/embeddings.npy

ğŸ” é¦–å—å†…å®¹éªŒè¯ï¼š
æ–‡æœ¬å†…å®¹ï¼š
This is the first sentence. Second sentence here. Third sentence ends.

åŒ…å«å¥å­æ•°ï¼š3
å®é™…å¥å­åˆ—è¡¨ï¼š
1. This is the first sentence.
2. Second sentence here.
3. Third sentence ends.

âœ… ç»“æ„åŒ–æ•°æ®å·²ä¿å­˜è‡³ output/chunks.json
"""