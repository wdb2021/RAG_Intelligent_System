2.2 DeepSeek-R1-Zero: 纯RL训练基础模型
内容概述：
作者详细描述了DeepSeek-R1-Zero阶段：
强化学习算法：使用Group Relative Policy Optimization (GRPO)来降低RL成本。GRPO不同
于传统PPO，不需要同等大小的价值网络，而是通过采样一组旧策略输出计算群组奖励作为基
线，大幅节省计算。公式(1)-(3)定义了优化目标与优势函数(advantage)的计算，基于每组样本
的相对奖励来更新策略。
奖励建模：采用规则驱动的奖励（非神经网络模型） ，包括准确性奖励（确保回答正
确，如数学答案框检验、代码编译测试）和格式奖励（确保推理过程用<think>标签包裹）
。他们没有使用学习型的过程或结果奖励模型，以避免大规模RL中的奖励偏差/漏洞
（reward hacking）及额外训练开销 。
训练模板：设计了简洁的训练模板，要求模型先输出推理过程( <think> 标签)、再输出答案
( <answer> 标签) 。这个模板只约束格式、不限定具体内容策略，以便观察RL
中模型自然演化，避免过多人为偏置 。
与前文的联系：
前面引出了DeepSeek-R1-Zero的RL方法，这一段具体展开如何实现：GRPO算法、奖励体系、训练
格式。它接续“方法概览”
，提供R1-Zero阶段的技术细节，并为后续性能结果做准备。
技术细节：
GRPO (Group Relative Policy Optimization): 这是核心算法，不需价值网络、用组样本算
baseline，提高效率。它沿袭PPO理念，但创新在Group Advantage计算 。
奖励设计：准确性和格式两个奖励模型，前者通过确定性验证(如数学有标准答案格式，代码编
译测试)直接给奖励 ；后者强制推理过程输出在指定标签中 。
ARXIV.ORG
ARXIV.ORG
ARXIV.ORG
ARXIV.ORG ARXIV.ORG
ARXIV.ORG
ARXIV.ORG
ARXIV.ORG ARXIV.ORG
3/23
Reward hacking: 提到了不用神经奖励模型，因为大模型RL易出现模型投机取巧使奖励最大化
但结果没意义 ，这一点非常关键，显示作者对奖励模型鲁棒性的重视。
模板：特制了User/Assistant对话形式，带<think>和<answer>标签 。此举统一了回
答格式，使训练和评估更规范，同时为模型留出了自我发挥空间 。
意义和亮点：
本段亮点包括：
创新RL算法应用：将更高效的GRPO用于LLM推理训练，展示在大模型上节省算力的重要思
路。
奖励体系稳健：纯规则奖励避免了复杂的奖励模型训练，减少副作用（如reward hacking）
。
格式引导：模板策略确保输出格式统一
，为后续自我迭代打下基础（比如确保模型的推理
Chain-of-Thought可读且标记清晰） ，这在之后R1模型的可读性改进中进一步体现。
扩展与思考：
可以思考GRPO为何适合这种场景？没有价值网络如何影响收敛？未来是否可引入更智能的奖励如
人类偏好模型？还有在不同任务设计简单统一的模板是否能让RL更有效？比如引导模型展示思路是
否普适？这些都值得进一步研究和验证。
2.2.4 DeepSeek-R1-Zero的性能与演化
内容概述：
这一段汇报了DeepSeek-R1-Zero的训练表现和过程中观察到的现象：
性能指标：表2展示了R1-Zero在多个推理基准上的成绩，与OpenAI-o1-mini和o1-0912对比。
DeepSeek-R1-Zero在AIME 2024达到71.0%、MATH-500 86.7%、GPQA Diamond 95.9%、
LiveCodeBench 73.3%，CodeForces rating 1444。虽然AIME等略低于o1-0912，但通过多数
投票(majority voting)可进一步提升AIME到86.7%，超越o1-0912。
训练曲线：图2绘制了R1-Zero在AIME上的准确率随RL步数稳步提升，从起初15.6%涨至
71.0%，验证RL算法的有效性。
涌现的推理能力：R1-Zero逐步学会延长思考链条（输出更多推理token） ，如图3所示，它在训
练中平均响应长度不断增长。这种自进化使模型自发运用更多思考时间解决复杂问题。
衍生行为：随着推理token增多，模型自然出现高级行为：如自省（reflection，回顾并修正之
前步骤）和多路径探索。这些都不是人为硬编码的，而是在RL奖励驱使下自发涌现。
ARXIV.ORG
ARXIV.ORG
ARXIV.ORG
ARXIV.ORG
4/23
“顿悟时刻”(aha moment)：训练中观察到模型会在中间阶段突然改变策略，如重新审视求解
方法，给人一种“灵光一闪”的感觉。表3举例说明一个中间模型版本在解题中突然停下来“Wait,
… aha moment”
，然后换种方式重新解决问题。这体现出RL可以引导模型自主调整思路。
缺陷：尽管推理强，R1-Zero也有明显问题，如可读性差（输出混杂多语言，缺少格式化） ，这
使得直接用于用户交互不够友好。为此，引出了下一步DeepSeek-R1来改进可读性和通用性。
与前文的联系：
前文讲了R1-Zero的训练细节，本段给出实验结果和训练观察。结果证明了前文策略的有效性，自演
化行为也与前面模板无内容约束的设计一致，支持前述决策。
技术细节：
基准测试：列出多个基准如AIME（数学竞赛） 、MATH-500（数学） 、GPQA Diamond（常识问
答） 、LiveCodeBench（编程实时评测） 、CodeForces（算法竞赛排名） 。Pass@1、
Cons@64(共识64样本多数投票)等指标用于评估。
Majority Voting：对每道题采样多次，让答案投票。这里Cons@64=多数投票64个样本的准确
率，是提升模型可靠性的重要技巧。
自演化：引入thinking time (推理token数) 概念，显示模型自动增加推理步骤。
Reflection：模型自省。在RL中不加特殊约束，却学会了回头检查和纠错。
“Aha moment”：一种质变时刻，表明模型策略转变。例子中模型停下来说“Wait, wait... let’s
reevaluate step by step”
，体现模型开始反思之前的解题路径。
缺陷：强调可读性问题，如多语言混用，没有MarkDown格式突出答案等。
意义和亮点：
性能近似SOTA：DeepSeek-R1-Zero仅靠RL，已接近OpenAI-o1系列的表现。无监督数据却能
达此成绩，极具意义，证明RL可大幅提升模型推理能力。
涌现行为：模型自发学会延长思考和反思，这是强AI特征。Reflection和多路径探索说明LLM
在适当激励下可以类似人类地思考，令人印象深刻。
“Aha moment”：这个现象不仅对模型有意义，对研究者也是惊喜，展示RL可能激发LLM产生
新颖解题策略。
发现问题：R1-Zero暴露的缺陷（如可读性）为下一步改进指明方向。通过暴露问题，凸显深入
改进的重要性。
5/23
扩展与思考：
这些发现引出一些思考：
Majority Voting等方法能否自动化融入推理过程，减少推理错误？
模型自省和顿悟是否可通过设计元认知模块加强？
R1-Zero的多语言混杂问题根源是什么？（可能是在纯RL过程中，模型为了提高准确率不自觉
调用了预训练语言能力，从而混用语言输出。 ）后续R1用语言一致性奖励来解决。
RL训练中监控涌现行为本身可成为研究课题：如何量化“aha moment”？这些行为在不同种子
或不同模型上是否普遍？
3.
DeepSeek-R1: 冷启动结合强化学习
3.3.1 冷启动 (Cold Start)
内容概述：
由于DeepSeek-R1-Zero存在初期不稳定和可读性差等问题，DeepSeek-R1引入**“冷启动数据”**
做预微调：
为避免纯RL一开始不稳定，作者收集/构造了几千条高质量的长链CoT数据对基础模型进行微
调，使之成为RL初始策略。
冷启动数据来源多样：包括Few-shot示例引导长CoT，直接提示模型生成带自我反思和验证的
长答案，利用R1-Zero已有输出经人工后处理整理可读性版本。
这些冷启动数据注重可读性：设计了输出模式如 |special
_
token|
<reasoning_process>|special
_
token|<summary> ，即回答由两部分构成——推理过程
(Chain-of-Thought)，以及末尾总结。通过筛选，确保没有多语言混杂、答案有清晰格式、对
用户友好。
冷启动数据优势：①可读性更强，输出有markdown或清晰格式；②更高潜力（带有人类先验
的模式）使后续模型性能胜过无冷启动的R1-Zero。
与前文的联系：
本段衔接R1-Zero的缺陷，提出解决方案冷启动。既回答了前文提出的改进方向（提高可读性、稳定
训练） ，也为后续R1训练各阶段埋下伏笔（后面还有RL和SFT阶段） 。
技术细节：
冷启动数据量：数千条，这与R1-Zero完全无监督对比鲜明。
6/23
获取方法：包括Few-shot长CoT、直接提示生成详细解答、利用R1-Zero的回答加以人工清洗
等。这些方法都侧重得到长且清晰的推理过程示例。
格式：新的输出格式引入了推理过程+总结分隔。这可以看作一个简单的“模板升级”：R1-Zero
模板强调<think>/<answer>，R1模板进一步要求总结，这是针对人类可读性调整。
special
_
token应该是一种标记界定推理与总结部分。
人类偏好：冷启动数据在设计上蕴含人类偏好(如总结、单一语言)，属于人工先验引导模型行为
的体现。
意义和亮点：
创新冷启动策略：以小数据提高初始状态，加速RL收敛并提升最终性能。这在大模型训练中节
省大量算力，是一种很务实的创新。
用户友好性：融合总结让输出直观，这对LLM落地应用非常重要。DeepSeek-R1更适合面向用
户，因为推理过程清晰、结果明确，对开发和最终用户都有价值。
表明RL与少量监督可互补：证明哪怕少量高质量数据，也能有效引导纯RL，使得模型更稳定、
更强大，这是一个可推广的经验。
扩展与思考：
数据构造成本：这些长CoT数据是如何高效构造的？引发对数据自动生成（如用已有强模型生
成）vs人工打磨的思考。
是否存在最佳比例：多少冷启动数据足够？多了会不会变成主要靠SFT而失去“纯RL”意义？
迭代训练：文中提及“我们认为迭代训练是更好的路径”(iterative training is better)，暗示未来
模型训练可交替进行多轮SFT和RL。这类似强化学习中**“curriculum learning”或“阶段训
练”**
，可进一步探讨最优策略。
多语言问题：R1主要用了中英文优化，对其他语言处理不好。未来或需加入更多语言的冷启动
数据或多语言一致性奖励，以扩展模型语言覆盖面。
2.3.2 推理导向的强化学习 (Reasoning-Oriented RL)
内容概述：
在用冷启动数据微调基础模型后，作者对DeepSeek-R1进行大规模推理导向RL：
RL过程重点放在推理密集的任务上，如编程、数学、科学、逻辑推理等。这些任务问题明确、
有确定性正确答案，便于评估和奖励。
7/23
训练中发现语言混用问题仍存在（尤其prompt含多种语言时，CoT会混杂中英） ，为此加入语
言一致性奖励。具体做法：计算推理过程（CoT）中目标语言词汇占比，作为奖励一部分。尽
管消除混用稍微降低性能，但换来可读性提升，更符合人类偏好。
最终总奖励 = 推理任务准确性奖励 + 语言一致性奖励（简单相加） 。然后在此奖励下，对微调
后的模型继续RL训练，直到推理任务上收敛。
与前文的联系：
紧接冷启动SFT后，这是R1训练的第二阶段（强化学习阶段） ，对比R1-Zero的RL，本段强调除了准
确性，还加了语言一致性，呼应了之前R1-Zero缺陷之一（语言混杂）的解决。也是接着回答引言提
出的如何进一步提升性能（通过小数据+RL提高收敛、性能） 。
技术细节：
RL任务聚焦：专挑Hard reasoning任务用RL训练，使模型主要在这些任务上突破。这类似对
模型进行专项训练，提高在推理类任务上的“肌肉”
。
语言一致性奖励：可以理解为一种正则项或对抗项，使模型输出单语推理过程。它的计算方式
应是CoT中目标语言（如English）单词数/总词数。消融实验发现加此项略降性能，但提升可读
性。这体现了性能-可读性的权衡：最终作者选择偏好人类可读性。
收敛：继续RL训练直到“在推理任务上收敛”
。文中没细说标准，但应该是像AIME/MATH这些基
准的pass@1不再提高等。
意义和亮点：
偏好集成：首次在大模型RL中明确加入人类可读性偏好指标，使模型结果更友好，贴近实用。
任务专注：通过强化模型在一类任务上的性能，显示RL的定制化能力：我们可以针对特定任务
族特别优化LLM，这对现实应用（比如专攻数学助理、编程助理）很有意义。
透明取舍：作者坦言加入语言一致性有性能代价，但仍执行，说明在产品化视角，可读性重
要。此透明也提醒读者：追求性能之外，模型可用性也是关键指标。
扩展与思考：
多目标RL：这里RL优化了准确性+语言一致性两个目标的加权和。未来可探索多目标优化或
Pareto优化，以更系统地平衡性能和可读性。
偏好奖励泛化：除了语言一致性，还有哪些“人类偏好”可加入RL?
比如减少重复、逻辑连贯等，
都可以尝试以规则或模型作为奖励。
8/23
语言一致性可能损性能，若要兼顾，多语言任务是不是需要分语言训练或语言标记？这涉及多
语言LLM训练更细的技巧。
2.3.3 拒绝采样与监督微调 (Rejection Sampling & SFT)
内容概述：
当第二阶段推理导向RL收敛后，进入第三阶段：用RL模型生成数据，扩充训练集并再进行SFT。具
体：
生成推理数据：从RL收敛的模型checkpoint出发，针对各种推理提示采样多个回答，通过拒绝
采样（rejection sampling）保留正确的推理过程和答案。在R1-Zero阶段只用规则可判定的数
据，这里拓展到更多类型：部分数据通过生成式奖励模型(如让DeepSeek-V3比较模型答案和真
实答案)来判断正误。同时，过滤掉难读的推理（混语言、段落过长、代码块杂乱等） 。总共收
集约60万条推理相关训练样本。
非推理数据：为提升模型的通用能力，还加入约20万条非推理领域的数据。这些来自
DeepSeek-V3的SFT集，包括写作、问答、自认知、翻译等任务。有趣的是，有的非推理任
务，作者也 prompt模型生成潜在CoT，但对简单问候等不会提供CoT。
将推理+非推理共约80万样本用于把基础模型（DeepSeek-V3-Base）再微调两轮（two
epochs） 。
此阶段的意义：结合RL生成的推理强数据和原有非推理数据，弥补模型在写作、角色扮演等方
面的弱项，同时保持推理能力，是综合能力提升的一步。
与前文的联系：
此段衔接第二阶段RL结束，说明如何进一步利用RL成果。同时响应了引言中第二个问题：如何训练
一个既推理强又通用的模型。通过加入非推理数据和再SFT，使R1具备强推理+广泛能力，为最终模
型奠定基础。
技术细节：
拒绝采样：核心技巧，在每个prompt采样多个输出，只保留正确的。这需要对每个输出有判
定。采用两种判定：
1.
规则：能自动判定正误的任务（如数学有标准答案，代码能跑测试）仍用规则奖励筛选
。
2.
生成式奖励：一些任务无法规则判定，就把真实答案和模型答案一起交给DeepSeek-V3模
型评估对错。这其实是用强模型当判别器。
ARXIV.ORG
9/23
数据规模：60万推理 + 20万非推理 = 80万数据用于SFT，这里较前一阶段几千条冷启动，数据
量大两个数量级。说明R1逐步构建出大规模数据来提升模型。
非推理数据：沿用DeepSeek-V3已有数据，表明作者复用前代模型成果。对于一些复杂非推理
任务，还引导模型想CoT，这也许能提升回答质量，尽管非推理任务不要求Chain-of-
Thought，但该策略可能帮助模型理清思路再答。
两轮微调：用80万数据训练两轮，估计是为了让模型充分学习新数据而不过拟。
意义和亮点：
数据自举(self-bootstrapping)：用模型自己生成大量高质量数据再训练自己，这是无监督强化
走向半监督/自监督的漂亮一招，验证了LLM自举的可能性。
通用能力补全：这一步确保DeepSeek-R1不只是个推理怪才，还能在日常任务胜任，使之更加
全面。
规模效应：从几千（冷启动）到80万（此阶段）数据，体现了规模带来的质变：借助模型自行
生成，可以迅速积累远超人工标注规模的数据，这对于大模型训练具有借鉴意义。
过滤和判别：展现了构造高质量数据的严谨——不仅大量，还确保正确和可读。这保证了后续
微调效果，值得称赞。
扩展与思考：
模型自生成数据的边界：DeepSeek-R1已有较强推理能力，用它生成数据应该靠谱，但若模型
偏弱时自生成可能引入噪音。如何判定模型足够强可用于数据生成？
DeepSeek-V3在此充当评估者角色，让人想到监督模型验证强化学习模型。未来是否可以联合
训练这种评估模型提高准确性？
作者用了DeepSeek-V3已有数据，对于没现成数据的任务，是否能一步步扩展？比如先在推理
任务上RL，生成推理数据，再引入更多对话或知识任务数据……这种阶段性自我完善路线或成
趋势。
Chain-of-Thought在通用任务的作用可以进一步思考。虽然有些简单任务不需要CoT，但培养
模型先思考再答可能对保持一致性有好处，当然也需防不必要的冗长。
2.3.4 全场景强化学习 (RL for All Scenarios)
内容概述：
DeepSeek-R1训练的最后阶段，是第二轮强化学习，旨在全方位对齐人类偏好，使模型既有推理
力，又有用(helpfulness)且无害(harmless)。做法：
10/23
多重奖励信号：结合推理和通用场景的奖励。对于推理类数据，仍用之前的规则奖励 (数学、代
码、逻辑)。对一般任务（如开放问答等） ，用偏好模型(reward models)捕捉人类偏好。这些偏
好模型和DeepSeek-V3流程一致，采用类似的偏好对比数据和提示分布进行训练。
帮助性：在算helpfulness奖励时，只看最终总结部分的回答质量。这样确保评价关注给用户的
答案有用性，而不干扰推理过程。
无害性：则评估整个响应（包括推理过程和总结） ，以捕捉任何潜在不良内容并惩罚。这样保证
模型在推理时不输出有害信息（哪怕在<think>里也不行） 。
多样提示：为了让模型适应各种场景，这轮RL用多样prompt分布训练，包括之前推理任务和
广泛的用户请求，以全面提升模型对不同场景的适应性。
最终通过这些奖励和数据分布，使模型在推理、帮助性、无害性三方面都得到优化，训练出更
平衡的DeepSeek-R1。
与前文的联系：
此阶段收尾整个DeepSeek-R1训练管线，将之前的推理能力与通用能力集合，贯彻安全对齐思想
（helpful & harmless） 。它承接第二阶段RL和第三阶段SFT结果，在此基础上进一步微调。这也呼
应Introduction对LLM后训练**“align with social values, adapt to user preferences”**的提及。
技术细节：
Reward models：DeepSeek-V3 pipeline已有偏好对比数据(preference pairs)和训练有素的
奖励模型来评估回答是否符合人类喜好。这里直接使用类似方案评价DeepSeek-R1输出。
Prompt多样性：训练中混合了推理类prompt和非推理prompt，保证模型在专业题和闲聊任
务都能兼顾。
Helpfulness评价：仅看总结，即 <summary> 部分。这其实假设用户只看最终答案，<think>部
分主要给模型推理用，但helpfulness不因此受干扰。
Harmlessness评价：覆盖整个输出，意味着推理过程如有不当言论也算扣分。这是防止模型
借推理过程之名输出问题内容的防线。
确保推理能力：虽然引入偏好模型，但对推理场景依然保留规则奖励，防止模型一味讨好用户
而削弱推理严谨性。这体现了两类奖励并重的策略。
意义和亮点：
全面对齐：这一阶段让DeepSeek-R1更实用安全，不仅会解题，还懂礼貌、安全，对于LLM实
际落地非常关键。
11/23
分段评价：提出对一个完整回答分部分计算不同偏好（帮助性看答案，无害性看全体） ，这是很
细致的做法，凸显作者对推理过程透明度与用户体验两者的兼顾。
继承前作经验：沿用DeepSeek-V3偏好训练框架，说明大模型对齐可以复用成熟方案，不必重
新造轮子，与推理强化训练相结合效果佳。
推理能力保持：作者并未因人类偏好而牺牲推理准确度，体现性能与安全并举。这种平衡是
LLM发展的重点之一
。
扩展与思考：
安全性在推理链中的挑战：如何确保<think>部分安全？未来可能需要检测或过滤模型推理链，
避免其中潜藏不当内容，但不过度影响模型推理质量。
对齐的局限：只是二次RL，不排除一些价值偏差或幻觉仍存在，如何进一步自动化检测可能的
有害输出？可探索**RLHF（人类反馈强化学习）**结合Chain-of-Thought，让人类直接干预
<think>的输出。
Prompt工程：提到DeepSeek-R1对few-shot提示非常敏感，最好零样本+格式明确地提问。这
也引发思考，未来模型能否更鲁棒地处理不同提示？Prompt敏感性或可通过对齐数据进一步优
化。
4.
蒸馏：赋能小模型推理 (Distillation to Smaller Models)
内容概述：
作者在构建DeepSeek-R1大模型后，探索将其推理能力蒸馏给小模型（参数范围1.5B到70B） 。他们
直接微调开源的小模型（如Qwen2.5系列、Llama系列）使用前面精心整理的80万样本。主要发
现：
这种简单蒸馏极大提升了小模型的推理能力。如Qwen2.5-7B、14B、32B等受训模型被称为
DeepSeek-R1-Distill-*
。他们未对蒸馏模型做额外RL，只是想展示蒸馏本身的效果，把带RL的
强化留给社区未来探索 。
选择的基础模型有数学优化过的Qwen2.5-Math系列(1.5B、7B、14B、32B)和Llama-3.x系列
(8B, 70B) 。采用Llama-3.3-Instruct是因为其推理略优于3.1版本 。
结果表明直接蒸馏就让小模型性能超过自己用RL训练的结果。比如后文表5显示：Distill-
Qwen-7B达到55.5% AIME，高于Qwen原32B模型(QwQ-32B-Preview只有50.0%)。14B蒸馏
模型全面超越QwQ-32B-Preview。32B、70B蒸馏模型更是逼近或超过OpenAI o1-mini在大多
数基准上。
ARXIV.ORG
ARXIV.ORG ARXIV.ORG
12/23
作者指出：对这些小模型如果进一步做RL会有显著额外提升，但限于篇幅与探索范围，他们只
报告纯SFT蒸馏结果。
与前文的联系：
这部分承接上文开放问题：小模型能否通过大规模RL自行达到相同水平？（4.1节讨论） 。蒸馏段提
供了解决方案：用大模型指导小模型，展现出小模型性能的大跃升，并为讨论提供证据。
技术细节：
蒸馏数据：前面§2.3.3和2.3.4产生的80万样本用来微调小模型。相当于知识转移。
未额外RL：作者明确此处只做监督微调（SFT） ，没有对小模型再跑GRPO等RL流程 。
这样可以将提升归因于蒸馏，而非另一套RL。
基座模型：Qwen2.5-Math系列指腾讯/Qwen在数学上微调过的版本，这本身说明基础模型已
经较擅长数学，再加上R1数据微调效果更好。Llama-3.3-70B-Instruct是最新Meta开源大模型
的指令微调版。挑选这些模型表示基座质量也很重要。
参数对比：蒸馏模型参数远小于DeepSeek-R1（DeepSeek-R1可能基于>100B参数模型，文中
暗示OpenAI-o1-1217参数非常大） 。但蒸馏后，7B-32B模型竟能接近甚至超过一些百亿模型的
成绩。
意义和亮点：
小模型大用：证明了蒸馏能让小模型获得大模型的推理模式，小模型也能“举重若轻”
。这对部
署成本和开源社区都是福音，因为小模型更易使用、计算量低。
开源贡献：文中说开放了1.5B, 7B, 8B, 14B, 32B, 70B的蒸馏模型给社区。这非常有价值，为研
究者和开发者提供现成强力模型。
蒸馏优于小模型自RL：后续讨论4.1进一步强调，小模型自己RL需要巨大算力且效果不如蒸
馏。因此蒸馏既经济又高效，特别适合资源有限的环境。
知识可传递：说明推理模式等高层知识可由大模型向小模型传递，这对理解模型表征也有启
发：大模型学到的Chain-of-Thought技能是可以迁移的。
扩展与思考：
RL+蒸馏：如果在小模型上再进行一些RL微调，会不会逼近大模型性能？如何平衡蒸馏和微
调？
不同领域蒸馏：此处蒸馏数据主要是推理类的，也混有通用任务。对于特定领域（如医学、法
律）的大模型，是否也可用类似蒸馏方法造出小领域专家模型？
ARXIV.ORG
13/23
知识上限：小模型参数少，是否有上限无法逼近大模型？文中7B模型虽大幅超越自己基座，但
离OpenAI-o1仍有差距，暗示模型大小仍限制最高性能。这与4.1节结论一致：突破智能上限还
需更强基座和更大规模RL。
开放 vs 封闭：DeepSeek-R1及蒸馏模型开源对比OpenAI-o1闭源，可以思考开源社区通过协
作和蒸馏，能否追上甚至超越封闭SOTA的可能性。